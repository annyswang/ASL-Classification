{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2682397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ujson\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540d594",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1242530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the sentences as one of the three types: polar, wh, statement\n",
    "def label_sentence_type(s):\n",
    "    if re.search(\"\\?\", s):\n",
    "        if re.search(\"\\?[a-zA-Z]\", s):\n",
    "            return \"statement\"\n",
    "        elif re.search(\"[wW](ho|hat|hen|here|hy|hich)|[hH]ow\", s):\n",
    "            return \"wh\"\n",
    "        else:\n",
    "            return \"polar\"\n",
    "    else:\n",
    "        return \"statement\"\n",
    "\n",
    "# columns for the y-coord of each eyebrow keypoint (left: 17-21, right: 22-26), subtracted by top nose (27)\n",
    "# one col for SENTENCE_NAME\n",
    "keypoints_range = (3 * pd.DataFrame.from_dict(list(range(17,28))) + 1)[0]\n",
    "\n",
    "# read the keypoint data for a given sentence\n",
    "def extract_keypoints(sentence_name):\n",
    "    folder = 'openpose_output/json/' + sentence_name + '/'\n",
    "    json_files = map(lambda file: folder + file, os.listdir(folder))\n",
    "    extractor = lambda file: pd.DataFrame(pd.read_json(file, encoding='unicode_escape', encoding_errors='replace')['people'].iloc[0]['face_keypoints_2d']).transpose()[keypoints_range]\n",
    "    kp = pd.concat(map(extractor, json_files), sort=False, ignore_index=True)\n",
    "    kp.columns = list(range(17,28))\n",
    "    kp['SENTENCE_NAME'] = sentence_name\n",
    "    return kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0082d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the combined csv file to load all of the sentence data\n",
    "csvname = 'how2sign_realigned_combined.csv'\n",
    "with open(csvname, 'r', encoding='utf8') as csvfile:\n",
    "    # construct the sentence data frame\n",
    "    sentence_data = pd.read_csv(csvfile, sep='\\t')\n",
    "    # label the sentences\n",
    "    sentence_data['SENTENCE_TYPE'] = sentence_data['SENTENCE'].apply(label_sentence_type)\n",
    "\n",
    "# list all of the json files (frame keypoint data)\n",
    "json_files = pd.DataFrame(data={'FOLDER_NAME': os.listdir('openpose_output/json/')})\n",
    "\n",
    "# remove any entry whose SENTENCE_NAME that doesn't have matching video data\n",
    "sentence_data = pd.merge(sentence_data, json_files['FOLDER_NAME'], how='inner', left_on='SENTENCE_NAME', right_on='FOLDER_NAME').drop(columns=['FOLDER_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ee3b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polar        456\n",
       "statement    450\n",
       "wh           435\n",
       "Name: SENTENCE_TYPE, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 450\n",
    "split_frac = 0.8\n",
    "seed = 42069\n",
    "\n",
    "# form working dataset: take 450 random statements and all of the non-statements (polar and wh)\n",
    "wd = pd.concat([sentence_data[sentence_data['SENTENCE_TYPE'] == 'statement'].sample(n=N, random_state=seed),\n",
    "                sentence_data[sentence_data['SENTENCE_TYPE'] != 'statement']], ignore_index=True)\n",
    "wd['SENTENCE_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00eaa944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIDEO_ID</th>\n",
       "      <th>VIDEO_NAME</th>\n",
       "      <th>SENTENCE_ID</th>\n",
       "      <th>SENTENCE_NAME</th>\n",
       "      <th>START_REALIGNED</th>\n",
       "      <th>END_REALIGNED</th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>SENTENCE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BfS0S717ykQ</td>\n",
       "      <td>BfS0S717ykQ-2-rgb_front</td>\n",
       "      <td>BfS0S717ykQ_8</td>\n",
       "      <td>BfS0S717ykQ_8-2-rgb_front</td>\n",
       "      <td>44.19</td>\n",
       "      <td>47.96</td>\n",
       "      <td>It will also help you grow hair by stimulating...</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1KKlZKlWdTM</td>\n",
       "      <td>1KKlZKlWdTM-5-rgb_front</td>\n",
       "      <td>1KKlZKlWdTM_15</td>\n",
       "      <td>1KKlZKlWdTM_15-5-rgb_front</td>\n",
       "      <td>102.71</td>\n",
       "      <td>107.43</td>\n",
       "      <td>And lastly your lip gloss, that the most impor...</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1bjHvgrcZu4</td>\n",
       "      <td>1bjHvgrcZu4-8-rgb_front</td>\n",
       "      <td>1bjHvgrcZu4_2</td>\n",
       "      <td>1bjHvgrcZu4_2-8-rgb_front</td>\n",
       "      <td>21.29</td>\n",
       "      <td>36.66</td>\n",
       "      <td>Basically, take some kind of wasp, bee killer ...</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15HbVwQP1Qw</td>\n",
       "      <td>15HbVwQP1Qw-5-rgb_front</td>\n",
       "      <td>15HbVwQP1Qw_16</td>\n",
       "      <td>15HbVwQP1Qw_16-5-rgb_front</td>\n",
       "      <td>104.52</td>\n",
       "      <td>120.34</td>\n",
       "      <td>You want to, you want to be careful with acne ...</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0yeKUlT9LnU</td>\n",
       "      <td>0yeKUlT9LnU-5-rgb_front</td>\n",
       "      <td>0yeKUlT9LnU_1</td>\n",
       "      <td>0yeKUlT9LnU_1-5-rgb_front</td>\n",
       "      <td>11.73</td>\n",
       "      <td>17.05</td>\n",
       "      <td>In soccer, it's very important for you to warm...</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>cw5evdziBB4</td>\n",
       "      <td>cw5evdziBB4-8-rgb_front</td>\n",
       "      <td>cw5evdziBB4_3</td>\n",
       "      <td>cw5evdziBB4_3-8-rgb_front</td>\n",
       "      <td>19.58</td>\n",
       "      <td>23.02</td>\n",
       "      <td>So Molinia do you care to demonstrate with me?</td>\n",
       "      <td>polar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>cw5evdziBB4</td>\n",
       "      <td>cw5evdziBB4-8-rgb_front</td>\n",
       "      <td>cw5evdziBB4_6</td>\n",
       "      <td>cw5evdziBB4_6-8-rgb_front</td>\n",
       "      <td>33.7</td>\n",
       "      <td>35.26</td>\n",
       "      <td>Can you swing your legs around?</td>\n",
       "      <td>polar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>eLv9Uhs89IQ</td>\n",
       "      <td>eLv9Uhs89IQ-8-rgb_front</td>\n",
       "      <td>eLv9Uhs89IQ_0_1</td>\n",
       "      <td>eLv9Uhs89IQ_0_1-8-rgb_front</td>\n",
       "      <td>1.87</td>\n",
       "      <td>8.16</td>\n",
       "      <td>Hello, have you ever wondered how to prepare f...</td>\n",
       "      <td>wh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>eahjYz2685g</td>\n",
       "      <td>eahjYz2685g-8-rgb_front</td>\n",
       "      <td>eahjYz2685g_14</td>\n",
       "      <td>eahjYz2685g_14-8-rgb_front</td>\n",
       "      <td>85.09</td>\n",
       "      <td>86.4</td>\n",
       "      <td>Ok, so what?</td>\n",
       "      <td>wh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>fE6xxSbjVV8</td>\n",
       "      <td>fE6xxSbjVV8-8-rgb_front</td>\n",
       "      <td>fE6xxSbjVV8_7</td>\n",
       "      <td>fE6xxSbjVV8_7-8-rgb_front</td>\n",
       "      <td>54.29</td>\n",
       "      <td>55.94</td>\n",
       "      <td>See how you have that all down there?</td>\n",
       "      <td>wh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1341 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VIDEO_ID               VIDEO_NAME      SENTENCE_ID  \\\n",
       "0     BfS0S717ykQ  BfS0S717ykQ-2-rgb_front    BfS0S717ykQ_8   \n",
       "1     1KKlZKlWdTM  1KKlZKlWdTM-5-rgb_front   1KKlZKlWdTM_15   \n",
       "2     1bjHvgrcZu4  1bjHvgrcZu4-8-rgb_front    1bjHvgrcZu4_2   \n",
       "3     15HbVwQP1Qw  15HbVwQP1Qw-5-rgb_front   15HbVwQP1Qw_16   \n",
       "4     0yeKUlT9LnU  0yeKUlT9LnU-5-rgb_front    0yeKUlT9LnU_1   \n",
       "...           ...                      ...              ...   \n",
       "1336  cw5evdziBB4  cw5evdziBB4-8-rgb_front    cw5evdziBB4_3   \n",
       "1337  cw5evdziBB4  cw5evdziBB4-8-rgb_front    cw5evdziBB4_6   \n",
       "1338  eLv9Uhs89IQ  eLv9Uhs89IQ-8-rgb_front  eLv9Uhs89IQ_0_1   \n",
       "1339  eahjYz2685g  eahjYz2685g-8-rgb_front   eahjYz2685g_14   \n",
       "1340  fE6xxSbjVV8  fE6xxSbjVV8-8-rgb_front    fE6xxSbjVV8_7   \n",
       "\n",
       "                    SENTENCE_NAME START_REALIGNED END_REALIGNED  \\\n",
       "0       BfS0S717ykQ_8-2-rgb_front           44.19         47.96   \n",
       "1      1KKlZKlWdTM_15-5-rgb_front          102.71        107.43   \n",
       "2       1bjHvgrcZu4_2-8-rgb_front           21.29         36.66   \n",
       "3      15HbVwQP1Qw_16-5-rgb_front          104.52        120.34   \n",
       "4       0yeKUlT9LnU_1-5-rgb_front           11.73         17.05   \n",
       "...                           ...             ...           ...   \n",
       "1336    cw5evdziBB4_3-8-rgb_front           19.58         23.02   \n",
       "1337    cw5evdziBB4_6-8-rgb_front            33.7         35.26   \n",
       "1338  eLv9Uhs89IQ_0_1-8-rgb_front            1.87          8.16   \n",
       "1339   eahjYz2685g_14-8-rgb_front           85.09          86.4   \n",
       "1340    fE6xxSbjVV8_7-8-rgb_front           54.29         55.94   \n",
       "\n",
       "                                               SENTENCE SENTENCE_TYPE  \n",
       "0     It will also help you grow hair by stimulating...     statement  \n",
       "1     And lastly your lip gloss, that the most impor...     statement  \n",
       "2     Basically, take some kind of wasp, bee killer ...     statement  \n",
       "3     You want to, you want to be careful with acne ...     statement  \n",
       "4     In soccer, it's very important for you to warm...     statement  \n",
       "...                                                 ...           ...  \n",
       "1336     So Molinia do you care to demonstrate with me?         polar  \n",
       "1337                    Can you swing your legs around?         polar  \n",
       "1338  Hello, have you ever wondered how to prepare f...            wh  \n",
       "1339                                       Ok, so what?            wh  \n",
       "1340              See how you have that all down there?            wh  \n",
       "\n",
       "[1341 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e8047c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>SENTENCE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296.711</td>\n",
       "      <td>294.382</td>\n",
       "      <td>294.382</td>\n",
       "      <td>295.935</td>\n",
       "      <td>299.429</td>\n",
       "      <td>296.711</td>\n",
       "      <td>292.052</td>\n",
       "      <td>289.723</td>\n",
       "      <td>290.111</td>\n",
       "      <td>293.605</td>\n",
       "      <td>311.077</td>\n",
       "      <td>BfS0S717ykQ_8-2-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272.412</td>\n",
       "      <td>267.796</td>\n",
       "      <td>265.950</td>\n",
       "      <td>267.334</td>\n",
       "      <td>269.181</td>\n",
       "      <td>269.181</td>\n",
       "      <td>265.488</td>\n",
       "      <td>265.488</td>\n",
       "      <td>268.719</td>\n",
       "      <td>273.335</td>\n",
       "      <td>283.952</td>\n",
       "      <td>BfS0S717ykQ_8-2-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>281.313</td>\n",
       "      <td>276.691</td>\n",
       "      <td>274.590</td>\n",
       "      <td>275.430</td>\n",
       "      <td>278.371</td>\n",
       "      <td>276.271</td>\n",
       "      <td>272.489</td>\n",
       "      <td>271.229</td>\n",
       "      <td>274.590</td>\n",
       "      <td>280.472</td>\n",
       "      <td>293.497</td>\n",
       "      <td>BfS0S717ykQ_8-2-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>286.144</td>\n",
       "      <td>281.063</td>\n",
       "      <td>278.098</td>\n",
       "      <td>278.522</td>\n",
       "      <td>281.486</td>\n",
       "      <td>278.098</td>\n",
       "      <td>274.287</td>\n",
       "      <td>273.440</td>\n",
       "      <td>274.711</td>\n",
       "      <td>280.639</td>\n",
       "      <td>295.461</td>\n",
       "      <td>BfS0S717ykQ_8-2-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>284.804</td>\n",
       "      <td>279.462</td>\n",
       "      <td>278.229</td>\n",
       "      <td>278.229</td>\n",
       "      <td>280.695</td>\n",
       "      <td>277.818</td>\n",
       "      <td>273.297</td>\n",
       "      <td>272.475</td>\n",
       "      <td>275.352</td>\n",
       "      <td>280.284</td>\n",
       "      <td>295.079</td>\n",
       "      <td>BfS0S717ykQ_8-2-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159034</th>\n",
       "      <td>262.350</td>\n",
       "      <td>256.261</td>\n",
       "      <td>255.754</td>\n",
       "      <td>258.291</td>\n",
       "      <td>261.843</td>\n",
       "      <td>258.291</td>\n",
       "      <td>252.202</td>\n",
       "      <td>249.158</td>\n",
       "      <td>246.621</td>\n",
       "      <td>250.680</td>\n",
       "      <td>271.991</td>\n",
       "      <td>fE6xxSbjVV8_7-8-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159035</th>\n",
       "      <td>262.701</td>\n",
       "      <td>256.232</td>\n",
       "      <td>256.232</td>\n",
       "      <td>259.218</td>\n",
       "      <td>263.199</td>\n",
       "      <td>261.706</td>\n",
       "      <td>255.237</td>\n",
       "      <td>251.257</td>\n",
       "      <td>250.261</td>\n",
       "      <td>253.247</td>\n",
       "      <td>275.141</td>\n",
       "      <td>fE6xxSbjVV8_7-8-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159036</th>\n",
       "      <td>262.953</td>\n",
       "      <td>257.183</td>\n",
       "      <td>257.183</td>\n",
       "      <td>260.549</td>\n",
       "      <td>263.914</td>\n",
       "      <td>261.030</td>\n",
       "      <td>255.740</td>\n",
       "      <td>251.413</td>\n",
       "      <td>249.490</td>\n",
       "      <td>252.855</td>\n",
       "      <td>275.454</td>\n",
       "      <td>fE6xxSbjVV8_7-8-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159037</th>\n",
       "      <td>262.318</td>\n",
       "      <td>256.284</td>\n",
       "      <td>255.781</td>\n",
       "      <td>258.295</td>\n",
       "      <td>261.815</td>\n",
       "      <td>258.295</td>\n",
       "      <td>252.261</td>\n",
       "      <td>248.741</td>\n",
       "      <td>246.730</td>\n",
       "      <td>250.753</td>\n",
       "      <td>271.871</td>\n",
       "      <td>fE6xxSbjVV8_7-8-rgb_front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159038</th>\n",
       "      <td>261.877</td>\n",
       "      <td>257.306</td>\n",
       "      <td>256.799</td>\n",
       "      <td>257.814</td>\n",
       "      <td>261.369</td>\n",
       "      <td>257.814</td>\n",
       "      <td>253.244</td>\n",
       "      <td>249.181</td>\n",
       "      <td>247.149</td>\n",
       "      <td>250.197</td>\n",
       "      <td>273.050</td>\n",
       "      <td>fE6xxSbjVV8_7-8-rgb_front</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159039 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             17       18       19       20       21       22       23  \\\n",
       "0       296.711  294.382  294.382  295.935  299.429  296.711  292.052   \n",
       "1       272.412  267.796  265.950  267.334  269.181  269.181  265.488   \n",
       "2       281.313  276.691  274.590  275.430  278.371  276.271  272.489   \n",
       "3       286.144  281.063  278.098  278.522  281.486  278.098  274.287   \n",
       "4       284.804  279.462  278.229  278.229  280.695  277.818  273.297   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "159034  262.350  256.261  255.754  258.291  261.843  258.291  252.202   \n",
       "159035  262.701  256.232  256.232  259.218  263.199  261.706  255.237   \n",
       "159036  262.953  257.183  257.183  260.549  263.914  261.030  255.740   \n",
       "159037  262.318  256.284  255.781  258.295  261.815  258.295  252.261   \n",
       "159038  261.877  257.306  256.799  257.814  261.369  257.814  253.244   \n",
       "\n",
       "             24       25       26       27              SENTENCE_NAME  \n",
       "0       289.723  290.111  293.605  311.077  BfS0S717ykQ_8-2-rgb_front  \n",
       "1       265.488  268.719  273.335  283.952  BfS0S717ykQ_8-2-rgb_front  \n",
       "2       271.229  274.590  280.472  293.497  BfS0S717ykQ_8-2-rgb_front  \n",
       "3       273.440  274.711  280.639  295.461  BfS0S717ykQ_8-2-rgb_front  \n",
       "4       272.475  275.352  280.284  295.079  BfS0S717ykQ_8-2-rgb_front  \n",
       "...         ...      ...      ...      ...                        ...  \n",
       "159034  249.158  246.621  250.680  271.991  fE6xxSbjVV8_7-8-rgb_front  \n",
       "159035  251.257  250.261  253.247  275.141  fE6xxSbjVV8_7-8-rgb_front  \n",
       "159036  251.413  249.490  252.855  275.454  fE6xxSbjVV8_7-8-rgb_front  \n",
       "159037  248.741  246.730  250.753  271.871  fE6xxSbjVV8_7-8-rgb_front  \n",
       "159038  249.181  247.149  250.197  273.050  fE6xxSbjVV8_7-8-rgb_front  \n",
       "\n",
       "[159039 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract keypoints for the working dataset\n",
    "kp = pd.concat(map(extract_keypoints, wd['SENTENCE_NAME']), sort=False, ignore_index=True)\n",
    "kp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c8246",
   "metadata": {},
   "source": [
    "# Simple max and average models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d9647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brow distances\n",
    "def brow_dist(kp):\n",
    "    pts = kp[['SENTENCE_NAME', 'SENTENCE_TYPE']].copy()\n",
    "    pts['d_in'] = ((kp[27] - kp[21]) + (kp[27] - kp[22])) / 2\n",
    "    pts['d_out'] = ((kp[27] - kp[18]) + (kp[27] - kp[25])) / 2\n",
    "    pts['d_avg'] = (pts['d_in'] + pts['d_out']) / 2\n",
    "    return pts\n",
    "\n",
    "# helper function to help determine the sentence type\n",
    "def min_label(pt):\n",
    "    if pt['dpo'] < pt['dwh']:\n",
    "        if pt['dpo'] < pt['dst']:\n",
    "            return 'polar'\n",
    "    else:\n",
    "        if pt['dwh'] < pt['dst']:\n",
    "            return 'wh'\n",
    "    return 'statement'\n",
    "\n",
    "# p is the test set keypoints\n",
    "def predictor(p, model, pts):\n",
    "    # apply the model to p\n",
    "    pts_p = model(p)\n",
    "\n",
    "    # calculate the distance between pts_p and polar question\n",
    "    pts_p['dpo'] = (pts_p['d_avg'] - pts.loc['polar', 'd_avg']).abs()\n",
    "    # calculate the distance between pts_p and wh question\n",
    "    pts_p['dwh'] = (pts_p['d_avg'] - pts.loc['wh', 'd_avg']).abs()\n",
    "    # calculate the distance between pts_p and statement\n",
    "    pts_p['dst'] = (pts_p['d_avg'] - pts.loc['statement', 'd_avg']).abs()\n",
    "    \n",
    "    # apply min_label to the 3 columns row-by-row (axis=1)\n",
    "    pts_p['PREDICT_SENTENCE_TYPE'] = pts_p[['dpo', 'dwh', 'dst']].apply(min_label, axis=1)\n",
    "    \n",
    "    return pts_p\n",
    "\n",
    "def simple_max(kp):\n",
    "    # calculate the ptserence score for every frame and take the max across the frames of each sentence\n",
    "    pts = brow_dist(kp)[['SENTENCE_NAME', 'SENTENCE_TYPE', 'd_avg']].groupby('SENTENCE_NAME').max()\n",
    "    # return 3 scores corresponding to the 3 sentence types\n",
    "    return pts\n",
    "\n",
    "def simple_average(kp):\n",
    "    # calculate the ptserence score for every frame and take the average across the frames of each sentence\n",
    "    pts = brow_dist(kp)[['SENTENCE_NAME', 'SENTENCE_TYPE', 'd_avg']].groupby('SENTENCE_NAME').mean(numeric_only=True)\n",
    "    # reinsert the sentence_type column\n",
    "    mapping = kp[['SENTENCE_NAME', 'SENTENCE_TYPE']].groupby('SENTENCE_NAME').apply(lambda x: x.iloc[0])\n",
    "    pts = pts.merge(mapping['SENTENCE_TYPE'], on='SENTENCE_NAME')\n",
    "    # return 3 scores corresponding to the 3 sentence types\n",
    "    return pts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ba43f",
   "metadata": {},
   "source": [
    "# Logistic regression max and average models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65eac3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract keypoint 27 (top of nose) from all other eyebrow points\n",
    "def sub_all(kp):\n",
    "    pts = kp.copy()\n",
    "    for i in range(17,27):\n",
    "        pts[i] = pts[27] - pts[i]\n",
    "    return pts\n",
    "\n",
    "def max_label(prob):\n",
    "    if prob['polar'] > prob['wh']:\n",
    "        if prob['polar'] > prob['statement']:\n",
    "            return 'polar'\n",
    "    else:\n",
    "        if prob['wh'] > prob['statement']:\n",
    "            return 'wh'\n",
    "    return 'statement'\n",
    "\n",
    "# train logistic regression models\n",
    "def train_lr(kp):\n",
    "    pts = sub_all(kp)\n",
    "    lr = LogisticRegression(max_iter=1000).fit(pts[range(17,27)], pts['SENTENCE_TYPE'])\n",
    "    return lr\n",
    "\n",
    "# prediction function for model 3 - logistic max\n",
    "def predict_logit_max(p, lr):\n",
    "    pts = sub_all(p)\n",
    "    sentence_types = lr.classes_\n",
    "    probs = lr.predict_proba(pts[range(17,27)])\n",
    "    probs = pts.merge(pd.DataFrame(data=probs, columns=sentence_types), left_index=True, right_index=True)\n",
    "    probs = probs[['SENTENCE_NAME', 'polar', 'statement', 'wh']].groupby('SENTENCE_NAME').max()\n",
    "    probs['PREDICT_SENTENCE_TYPE'] = probs.apply(max_label, axis=1)\n",
    "    return probs\n",
    "\n",
    "# prediction function for model 4 - logistic average\n",
    "def predict_logit_average(p, lr):\n",
    "    pts = sub_all(p)\n",
    "    sentence_types = lr.classes_\n",
    "    probs = lr.predict_proba(pts[range(17,27)])\n",
    "    probs = pts.merge(pd.DataFrame(data=probs, columns=sentence_types), left_index=True, right_index=True)\n",
    "    probs = probs[['SENTENCE_NAME', 'polar', 'statement', 'wh']].groupby('SENTENCE_NAME').mean(numeric_only=True)\n",
    "    probs['PREDICT_SENTENCE_TYPE'] = probs.apply(max_label, axis=1)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7af21",
   "metadata": {},
   "source": [
    "# Train all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25d1b0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL0</th>\n",
       "      <th>simple_max</th>\n",
       "      <th>simple_average</th>\n",
       "      <th>logit_max</th>\n",
       "      <th>logit_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.401487</td>\n",
       "      <td>0.442379</td>\n",
       "      <td>0.360595</td>\n",
       "      <td>0.368030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.386617</td>\n",
       "      <td>0.427509</td>\n",
       "      <td>0.338290</td>\n",
       "      <td>0.345725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.371747</td>\n",
       "      <td>0.438662</td>\n",
       "      <td>0.342007</td>\n",
       "      <td>0.353160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.397770</td>\n",
       "      <td>0.427509</td>\n",
       "      <td>0.356877</td>\n",
       "      <td>0.349442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.353160</td>\n",
       "      <td>0.408922</td>\n",
       "      <td>0.338290</td>\n",
       "      <td>0.353160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.405204</td>\n",
       "      <td>0.431227</td>\n",
       "      <td>0.338290</td>\n",
       "      <td>0.338290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.338290</td>\n",
       "      <td>0.368030</td>\n",
       "      <td>0.345725</td>\n",
       "      <td>0.356877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.442379</td>\n",
       "      <td>0.338290</td>\n",
       "      <td>0.342007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.408922</td>\n",
       "      <td>0.438662</td>\n",
       "      <td>0.349442</td>\n",
       "      <td>0.356877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.334572</td>\n",
       "      <td>0.375465</td>\n",
       "      <td>0.453532</td>\n",
       "      <td>0.345725</td>\n",
       "      <td>0.353160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MODEL0  simple_max  simple_average  logit_max  logit_average\n",
       "0  0.334572    0.401487        0.442379   0.360595       0.368030\n",
       "1  0.334572    0.386617        0.427509   0.338290       0.345725\n",
       "2  0.334572    0.371747        0.438662   0.342007       0.353160\n",
       "3  0.334572    0.397770        0.427509   0.356877       0.349442\n",
       "4  0.334572    0.353160        0.408922   0.338290       0.353160\n",
       "5  0.334572    0.405204        0.431227   0.338290       0.338290\n",
       "6  0.334572    0.338290        0.368030   0.345725       0.356877\n",
       "7  0.334572    0.334572        0.442379   0.338290       0.342007\n",
       "8  0.334572    0.408922        0.438662   0.349442       0.356877\n",
       "9  0.334572    0.375465        0.453532   0.345725       0.353160"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use StratifiedShuffleSplit to split each of the types into training and test sets\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=1-split_frac, train_size=split_frac, random_state=seed)\n",
    "splitted = sss.split(wd, wd['SENTENCE_TYPE'])\n",
    "acc = pd.DataFrame()\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for i, (train, test) in enumerate(splitted):\n",
    "    # label the working data set with the train/test splits\n",
    "    # 'SET' indicates whether it's in train or test\n",
    "    wd.loc[train,'SET'] = 'train'\n",
    "    wd.loc[test,'SET'] = 'test'\n",
    "    \n",
    "    # join keypoint data to the train set\n",
    "    kp_train = kp.merge(wd[wd['SET'] == 'train'][['SET', 'SENTENCE_NAME', 'SENTENCE_TYPE']], on='SENTENCE_NAME')\n",
    "    # join keypoint data to the test set\n",
    "    kp_test = kp.merge(wd[wd['SET'] == 'test'][['SET', 'SENTENCE_NAME', 'SENTENCE_TYPE']], on='SENTENCE_NAME')\n",
    "\n",
    "    # 0. Basline model that maxes keypoint 17 over a sentence (just to group the data by sentence_name)\n",
    "    # but then ignores that keypoint entirely and always guesses statement\n",
    "    results0 = kp_test.copy()[[17, 'SENTENCE_NAME', 'SENTENCE_TYPE']].groupby('SENTENCE_NAME').max()\n",
    "    # null hypthesis: always guess 'statement'\n",
    "    results0['PREDICT_SENTENCE_TYPE'] = 'statement'\n",
    "    # accuracy\n",
    "    acc.loc[i, 'MODEL0'] = metrics.accuracy_score(results0['PREDICT_SENTENCE_TYPE'],results0['SENTENCE_TYPE'])\n",
    "    \n",
    "    # 1. Simple max\n",
    "    # train\n",
    "    predictor1 = lambda p: predictor(p, simple_max, simple_max(kp_train).groupby('SENTENCE_TYPE').mean())\n",
    "    # test\n",
    "    results1 = predictor1(kp_test)\n",
    "    # accuracy\n",
    "    acc.loc[i, 'simple_max'] = metrics.accuracy_score(results1['PREDICT_SENTENCE_TYPE'],results1['SENTENCE_TYPE'])\n",
    "    \n",
    "    # 2. Simple average\n",
    "    # train\n",
    "    predictor2 = lambda p: predictor(p, simple_average, simple_average(kp_train).groupby('SENTENCE_TYPE').mean(numeric_only=True))\n",
    "    # test \n",
    "    results2 = predictor2(kp_test)\n",
    "    # accuracy\n",
    "    acc.loc[i, 'simple_average'] = metrics.accuracy_score(results2['PREDICT_SENTENCE_TYPE'],results2['SENTENCE_TYPE'])\n",
    "    \n",
    "    # train logistic regression models\n",
    "    lr = train_lr(kp_train)\n",
    "    \n",
    "    # 3. Logistic max\n",
    "    # test\n",
    "    results3 = wd[wd['SET'] == 'test'][['SENTENCE_NAME', 'SENTENCE_TYPE']].merge(predict_logit_max(kp_test, lr), on='SENTENCE_NAME')\n",
    "    # accuracy\n",
    "    acc.loc[i, 'logit_max'] = metrics.accuracy_score(results3['PREDICT_SENTENCE_TYPE'],results3['SENTENCE_TYPE'])\n",
    "    \n",
    "    # 4. Logistic average\n",
    "    # test\n",
    "    results4 = wd[wd['SET'] == 'test'][['SENTENCE_NAME', 'SENTENCE_TYPE']].merge(predict_logit_average(kp_test, lr), on='SENTENCE_NAME')\n",
    "    # accuracy\n",
    "    acc.loc[i, 'logit_average'] = metrics.accuracy_score(results4['PREDICT_SENTENCE_TYPE'],results4['SENTENCE_TYPE'])\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b5ab4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MODEL0            0.334572\n",
       "simple_max        0.377323\n",
       "simple_average    0.427881\n",
       "logit_max         0.345353\n",
       "logit_average     0.351673\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
